{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Polynomial regression, and overfitting and underfitting\n",
    "\n",
    "This lab introduces polynomial regression.  Polynomial regression is similar to linear regression, but the data is modeled with a polynomial equation instead of a line.  Two problems that need to be considered with polynomial regression are overfitting and underfitting, and we will look at ways to detect them, including splitting the data into testing and training data. \n",
    "\n",
    "We will use simulated data for the first part of this lab to be able to easily illustrate different over- and under-fitting scenarios.  We will apply polynomial regression to a real world data set in the final section of this lab.\n",
    "\n",
    "### Section 1: Simulate the data\n",
    "\n",
    "First import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a polynomial equation has the form\n",
    "$$y = c_nx^n + c_{n-1}x^{n-1} + ... + c_3x^3 + c_2x^2 + c_1x + c_0$$\n",
    "where the $c_i$'s are the *coefficients*, which are real numbers.\n",
    "\n",
    "The highest exponent is the *degree* of the polynomial.\n",
    "\n",
    "For example, $y = -4x^2 +3x -5$ is a degree 2 polynomial and a parabola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,100)\n",
    "y = -4*x**2 + 3*x - 5\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, how did we compute $x^2$?\n",
    "\n",
    "The equation $$y = 2x^3 +3x^2 + 7x - 1.2$$ is a degree 3 polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,100)\n",
    "y = 2*x**3 + 3*x**2 +7*x - 1.2\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A degree 1 polynomial, like $y = 4.3x -2$, is a line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,100)\n",
    "y = 4.3*x - 2\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now simulate some data, which will we store in the DataFrame `df`.  Run the code below to do this.  It is ok if you do not understand this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of data points\n",
    "n = 30\n",
    "\n",
    "# variance of noise added to data\n",
    "noise_std = 0.3\n",
    "\n",
    "x_data = np.linspace(-5, 5, n)\n",
    "\n",
    "# define a function relating input to output\n",
    "f = lambda x: 0.1 * x**2\n",
    "\n",
    "# generate noisy data from the function\n",
    "\n",
    "y_data = f(x_data) + np.random.normal(0, noise_std, n)\n",
    "\n",
    "data = {\"x\": x_data, \"y\": y_data}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a scatterplot of the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shape does the simulated data have?  Would linear regression be a good model to use?  Why or why not?\n",
    "\n",
    "### Section 2: Polynomial regression\n",
    "\n",
    "Our simulated data has a parabolic shape to it, so let's try modeling it with a degree 2 polynomial.\n",
    "\n",
    "First we need to take our `x` column and square it (or raise it to the power 2):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "x_poly = polynomial_features.fit_transform(df[[\"x\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use `df[[\"x\"]]` instead of `df[\"x\"]` because we need to pass a DataFrame into the `fit_transform` function instead of a Series.  \n",
    "\n",
    "Display the new variable `x_poly`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original x values are in the middle column.  How were the first and third columns computed?\n",
    "\n",
    "The first column is $x^0 = 1$ for all values of $x$.  The third column is $x^2$.\n",
    "\n",
    "Now we create a new `LinearRegression` object.  Why linear regression if we are doing polynomial regression?  Because polynomial regression is just linear regression with the independent variables raised to different powers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_poly, df[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use the fitted model to predict the y values, so we can compare them with the true y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replot the scatterplot of the original simulated data.  Then add the following line of code to also plot the degree 2 polynomial model we just fitted:\n",
    "`plt.plot(df[\"x\"],y_pred)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think this model fits the data well?  Would you trust predictions made by this model?  Why or why not?\n",
    "\n",
    "Compute the Mean Squared Error (MSE) between the true y values and the predicted y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "    <code>mean_squared_error(df[\"y\"], y_pred)</code>\n",
    "</details>\n",
    "\n",
    "### Section 3: Under-fitting\n",
    "\n",
    "By itself the mean squared error is hard to interpret.  But let's compare it to the mean squared error when we fit a line (aka do linear regression) to the data.\n",
    "\n",
    "Use scikit-learn to fit a linear regression model (degree 1 polynomial) to our simulated data, and predict the y values.  \n",
    "\n",
    "Hint 1:  See Section 5 of Lab 9.\n",
    "\n",
    "Hint 2:  As above, use `df[[\"x\"]]` for the x data and `df[\"y\"]` for the y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(df[[\"x\"]],df[\"y\"])\n",
    "y_linear_pred = linear_regressor.predict(df[[\"x\"]])\n",
    "</code></details>\n",
    "\n",
    "Plot this linear model on a scatterplot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "sns.relplot(x = \"x\", y = \"y\", data = df)\n",
    "plt.plot(df[\"x\"],y_linear_pred)\n",
    "</code></details>\n",
    "\n",
    "How well does this linear model fit the data?  Which is a better model - the linear one or the degree 2 polynomial one?\n",
    "\n",
    "Do you think the mean squared error for this linear model will be higher or lower than the mean squared error for the degree 2 polynomial model?  Compute the mean squared error for the linear model to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error is a lot higher.  This is because the linear model *underfits* the data, meaning the model is too simple compared to the data. The linear model is not capturing the curve of the data.\n",
    "\n",
    "### Section 4: Over-fitting\n",
    "\n",
    "Every time we increase the degree of a polynomial, we can get another curve in its plot.  That is, the higher the degree of a polynomial, the curvier its plot can be.  We might think a higher degree polynomial model is better, because we have more flexibility to capture the trends in the data.  But with a higher degree polynomial we can have *over-fitting*, which occurs when the model is more complex than it needs to be.  In this case, the model will be reacting to random variation in the data, which we should ignore.\n",
    "\n",
    "Let's see an example of over-fitting.  First, create a degree 10 polynomial model for our data and make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polynomial_features= PolynomialFeatures(degree=10)\n",
    "x_poly10 = polynomial_features.fit_transform(df[[\"x\"]])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly10, df[\"y\"])\n",
    "y_poly10_pred = model.predict(x_poly10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the predicted model on the scatterplot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "sns.relplot(x = \"x\", y = \"y\", data = df)\n",
    "plt.plot(df[\"x\"],y_poly10_pred)\n",
    "</code>\n",
    "</details>\n",
    "\n",
    "How does the degree 10 polynomial model compare to the degree 2 polynomial model?  Which do you think is better?\n",
    "\n",
    "Finally, compute the mean squared error for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "mean_squared_error(y, y_poly10_pred)\n",
    "</code>\n",
    "</details>\n",
    "\n",
    "The degree 10 polynomial model has a lower mean squared error than either the linear or degree 2 polynomial model, so we might think it's the best.  But we are computing the mean squared error using the same data we trained our model with.  Instead, let's generate some new data from the same population (distribution) as the original data to test our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of data points\n",
    "n = 30\n",
    "\n",
    "# variance of noise added to data\n",
    "noise_std = 0.3\n",
    "\n",
    "x_data = np.linspace(-5, 5, n)\n",
    "\n",
    "# define a function relating input to output\n",
    "f = lambda x: 0.1 * x**2\n",
    "\n",
    "# generate noisy data from the function\n",
    "\n",
    "y_data = f(x_data) + np.random.normal(0, noise_std, n)\n",
    "\n",
    "data = {\"x\": x_data, \"y\": y_data}\n",
    "df_test = pd.DataFrame(data)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the required powers of x for the test data and make predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_poly10 = polynomial_features.fit_transform(df_test[[\"x\"]])\n",
    "\n",
    "y_test_pred = model.predict(x_test_poly10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the mean squared error for these predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "mean_squared_error(df_test['y'], y_test_pred)\n",
    "</code>\n",
    "</details>\n",
    "\n",
    "What happened to the mean squared error on the test data compared to when we made predictions on the original data?\n",
    "\n",
    "Plot the model predictions on the scatterplot of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.relplot(x = \"x\", y = \"y\", data = df_test)\n",
    "plt.plot(df[\"x\"],y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "sns.relplot(x = \"x\", y = \"y\", data = df_test)\n",
    "plt.plot(df[\"x\"],y_test_pred)\n",
    "</code>\n",
    "</details>\n",
    "\n",
    "Notice that the solid line representing the model is the same as the previous scatterplot, but the data points have changed position.  This is because we are using the same model, but on new data.  We can see the model doesn't fit this new data as well.\n",
    "\n",
    "This is an example of overfitting, because the equations for the lines are more complex than they need to be.  An over-fitted model does not handle new data as well, because it is trying to capture random variations in the data it was fit on, rather than only trying to capture the overall trend of the data.\n",
    "\n",
    "Since our data was simulated, we could just simulate more data to test the model on.  In the next lab, we will see how to split a data set (that cannot be easily increased) into training and testing data.\n",
    "\n",
    "### Section 5:  Polynomial regression on real data\n",
    "\n",
    "Let's apply polynomial regression to a real (not simulated) data set.  The following data set was downloaded from Kaggle [here](https://www.kaggle.com/aungpyaeap/fish-market) and is based on measurements of 159 fish caught from the same lake (Laengelmavesi) near Tampere, Finland.  The data was originally gathered in 1917.\n",
    "\n",
    "Data URL:  [https://raw.githubusercontent.com/megan-owen/MAT328-Techniques_in_Data_Science/main/data/Fish.csv](https://raw.githubusercontent.com/megan-owen/MAT328-Techniques_in_Data_Science/main/data/Fish.csv)\n",
    "\n",
    "First, load the data into a DataFrame called `fish`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a scatterplot with the length of the fish from nose to the beginning of the tail (Length1 column) on the x axis and the weight of the fish on the y axis.  Color the scatterplot by the fish species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the plot?  What is the relationship between weight and length1?\n",
    "\n",
    "#### Section 4.1:  Linear regression model for fish data\n",
    "\n",
    "Use scikit-learn to compute a linear regression model of this relationship (so predicting weight from length1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your linear regresison model to predict the weight for all fish using just their length1 measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your linear regression model line on the lenght1 vs. weight scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean squared error of these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the linear model is a good fit for the data?\n",
    "\n",
    "Now we will compare the linear model to a degree 2 polynomial model.\n",
    "\n",
    "#### Section 4.2: Polynomial regression model for fish data\n",
    "\n",
    "First, create a polynomial regression model of degree 2 to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this polynomial regresison model to predict the weight for all fish using just their length1 measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the degree 2 polynomial regression model using `plt.plot()` we will get zig-zagging lines because it plots the points in the order they are the data set.  We can fix this either by sorting first, or just use the `regplot()` function in Seaborn with the parameter `order = 2`, to indicate the regression line should be a degree 2 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean squared error of these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is better:  the linear regression or degree 2 polynomial regression model?  Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
