{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zW0edWQI3R3L"
   },
   "source": [
    "# Lab 16: Decision tree and random forest classifiers\n",
    "\n",
    "Decision trees can also be used for classification, or predicting the category a piece of data falls in.  \n",
    "\n",
    "We will first look at using decision trees to predict the location of vegetation on satellite images.\n",
    "\n",
    "Sections 1-3 of this lab are adapted from IACA Data Science Winter Pedagogy Workshop by Weiwei Pan and David Sondak (the [Day 3: Classification - decision trees, random forests and k-NN classifiers notebook](https://deepnote.com/project/3-more-classification-qkzrkkMjSYW8BqqN9u5Fyg/%2F05_classification_trees_and_forests.ipynb))\n",
    "\n",
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "mA3zyxCO3R3O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK3bW5Y33R3P"
   },
   "source": [
    "### Section 1: Monitoring changes in land cover using satellite images\n",
    "\n",
    "Satellite images can be used to detect changes in land cover. For example, the images below show how the vegetation cover around Belize and Guatemala changed from 1975 to 2007:\n",
    "\n",
    "<img src=\"http://www.terra-i.org/magnoliaAuthor/dam/jcr:b0a9f05a-e8a3-457b-9df2-eb10d921c69d/33_border_guatemala_1975_2007_full.jpg\">\n",
    "\n",
    "We will look at labelled data that has been generated by hand.  The first two columns contain normalized (scaled to have a mean of 0 and variance of 1) latitude and longitude values.  The third column indicates whether that locaiton contains vegetation, with 1 for vegetation and a 0 if not.  We want to use this data to predict whether a new location will contain vegetation.\n",
    "\n",
    "Data URL: https://raw.githubusercontent.com/onefishy/rwanda_2019/master/dataset_2.txt\n",
    "\n",
    "Open the data in Jupyter notebook.  What do you notice about the file?\n",
    "\n",
    "While the columns are separated by commas, there are no column headers.  The first two columns are the normalized location coordinates, and the third column is a 0 if there is no vegetation at that location and a 1 if there is vegetation at that location.\n",
    "\n",
    "Can you figure out how to read in the data, giving the columns the names `x_coord`, `y_coord`, and `veg`?  Hint:  See Lab 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pcnqWHtv3R3P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYLqG2IP3R3Q"
   },
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/onefishy/rwanda_2019/master/dataset_2.txt\", header = None, names = [\"x_coord\", \"y_coord\", \"veg\"])\n",
    "</code>\n",
    "</details>\n",
    "\n",
    "Plot the data as a scatterplot (with `x_coord` on the x axis and `y_coord` on the y axis) colored by the `veg` column.  If you are using Seaborn `relplot()`, you can color the vegetation green and the lack of vegetation brown with the parameter `palette = [\"brown\",\"green\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "Bjw3L2B33R3Q",
    "outputId": "3443a3fc-9b97-4524-e5f2-c532077e903f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5yREL5J3R3R"
   },
   "source": [
    "What do you notice about the location and shape of the vegetation?\n",
    "\n",
    "We will also use the scikit learn package for decision tree classifiers, so we need to prepare the data in the same way:\n",
    "* create a variable `x` containing the first two columns, which will be used to train the model and make the predictions\n",
    "* create a variable `y` containing the third column, which is the variable we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "A-7AYu8t3R3R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCc8umRr3R3R"
   },
   "source": [
    "Finally, split the data into training (80%) and testing (20%) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "k-P4EUIV3R3R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY__McrE3R3S"
   },
   "source": [
    "### Section 2: Decision tree classifiers\n",
    "\n",
    "We will fit a decision tree classifier to the training data with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dKZ5Kes3R3S",
    "outputId": "068ab4d7-b13b-4835-9a75-66dbdc1ce3b8"
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth = 5)\n",
    "tree.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_SV9r713R3S"
   },
   "source": [
    "Can you figure out how to use this model to predict whether there is a vegetation at each coordinate in the test data?  \n",
    "\n",
    "Hint:  All scikit-learn machine learning models use the same code functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pH2GXq-j3R3T",
    "outputId": "e6a5ae56-6127-49a8-e2cb-49a7a07e5ca4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRfT3cBY3R3T"
   },
   "source": [
    "<details><summary>Answer:</summary>\n",
    "y_test_preds = tree.predict(x_test)\n",
    "</details>\n",
    "\n",
    "We can also generate a confusion matrix to evaluate the classification with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMCmobKh3R3U",
    "outputId": "6805fc6b-e772-449a-80fb-4432a96c9d26"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx7lBjJr3R3U"
   },
   "source": [
    "As long as the true values are the first parameter and the predictions are the second parameter, the confusion matrix will have the same format as the one generated by `statsmodels`:\n",
    "\n",
    "<code>   \n",
    "                        predicted\n",
    "             |    0           |      1      |\n",
    "             --------------------------------\n",
    "observed | 0 | true negative  | false positive\n",
    "         | 1 | false negative | true positive\n",
    "</code>\n",
    "\n",
    "If the categories are something other than 0 and 1, they will be ordered in alphabetically order (ie. the first row is the category closest to a in the alphabet).\n",
    "\n",
    "Which does our decision tree predict the location of better:  vegetation or no vegetation?\n",
    "\n",
    "### Section 3: Understanding decision trees through visualization\n",
    "\n",
    "Let's visualize the predictions being made by the decision tree.\n",
    "\n",
    "The following function will plot a scatter plot of the data overlaided with the *decision tree boundary*, which shows what each coordinate will be predicted as.  You do not have to understand the details of the function (it uses MatPlotLib functions that we have not learned yet).\n",
    "\n",
    "Run this code to load the functions into the notebook so we can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZBufomYr3R3U"
   },
   "outputs": [],
   "source": [
    "def scatter_plot_data(x_df, y_series, ax):\n",
    "    '''\n",
    "    scatter_plot_data scatter plots the satellite data. A point in the plot is colored 'green' if \n",
    "    vegetation is present and 'gray' otherwise.\n",
    "    \n",
    "    input:\n",
    "       x_df - a DataFrame of size N x 2, each row is a location, each column is a coordinate\n",
    "       y_series - a Series of length N, each entry is either 0 (no vegetation) or 1 (vegetation)\n",
    "       ax - axis to plot on\n",
    "    returns: \n",
    "       ax - the axis with the scatter plot\n",
    "    '''\n",
    "    \n",
    "    # convert x_df and y_series into numpy arrays\n",
    "    x = x_df.values\n",
    "    y = y_series.values\n",
    "    \n",
    "    ax.scatter(x[y == 1, 0], x[y == 1, 1], alpha=0.2, c='green', label='vegetation')\n",
    "    ax.scatter(x[y == 0, 0], x[y == 0, 1], alpha=0.2, c='gray', label='nonvegetation')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlabel('Latitude')\n",
    "    ax.set_ylabel('Longitude')\n",
    "    ax.legend(loc='best')\n",
    "    return ax\n",
    "\n",
    "def plot_decision_boundary(x_df, y_series, model, ax, plot_boundary_only=False):\n",
    "    '''\n",
    "    plot_decision_boundary plots the training data and the decision boundary of the classifier.\n",
    "    input:\n",
    "       x_df - a DataFrame of size N x 2, each row is a location, each column is a coordinate\n",
    "       y_series - a Series of length N, each entry is either 0 (non-vegetation) or 1 (vegetation)\n",
    "       model - the 'sklearn' classification model\n",
    "       ax - axis to plot on\n",
    "       poly_degree - the degree of polynomial features used to fit the model\n",
    "    returns: \n",
    "       ax - the axis with the scatter plot\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # convert x_df and y_series into numpy arrays\n",
    "    x = x_df.values\n",
    "    y = y_series.values\n",
    "    \n",
    "    # Plot data\n",
    "    if not plot_boundary_only:\n",
    "        ax.scatter(x[y == 1, 0], x[y == 1, 1], alpha=0.2, c='green', label='vegetation')\n",
    "        ax.scatter(x[y == 0, 0], x[y == 0, 1], alpha=0.2, c='gray', label='non-vegetation')\n",
    "    \n",
    "    # Create mesh\n",
    "    interval = np.arange(0,1,0.01)\n",
    "    n = np.size(interval)\n",
    "    x1, x2 = np.meshgrid(interval, interval)\n",
    "    x1 = x1.reshape(-1, 1)\n",
    "    x2 = x2.reshape(-1, 1)\n",
    "    xx = np.concatenate((x1, x2), axis=1)\n",
    "    \n",
    "    # Predict on mesh points\n",
    "    yy = model.predict(xx)    \n",
    "    yy = yy.reshape((n, n))\n",
    "\n",
    "    # Plot decision surface\n",
    "    x1 = x1.reshape(n, n)\n",
    "    x2 = x2.reshape(n, n)\n",
    "    if not plot_boundary_only:\n",
    "        ax.contourf(x1, x2, yy, alpha=0.1, cmap='Greens')\n",
    "    ax.contour(x1, x2, yy, colors='black', linewidths=0.1)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlabel('Latitude')\n",
    "    ax.set_ylabel('Longitude')\n",
    "    ax.legend(loc='best')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUdjbVr93R3V"
   },
   "source": [
    "First, let's use this code to plot the decision tree boundary of the decision tree we just fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "r3l4-yzG3R3V",
    "outputId": "ba86919f-77cc-4727-b897-7239469e9852"
   },
   "outputs": [],
   "source": [
    "# set up to create two plots in the same image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# visualize the data on the first plot (ax[0])\n",
    "scatter_plot_data(x_train, y_train, ax[0])\n",
    "ax[0].set_title('Training Data')\n",
    "\n",
    "#  plot the training data and decision tree boundary on the second plot (ax[1])\n",
    "plot_decision_boundary(x_train, y_train, tree, ax[1])\n",
    "ax[1].set_title('Decision Boundary on the Training Data')\n",
    "\n",
    "#  plot the test data and decision tree boundary on the third plot (ax[2])\n",
    "plot_decision_boundary(x_test, y_test, tree, ax[2])\n",
    "ax[2].set_title('Decision Boundary on the Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4D5IAEk3R3W"
   },
   "source": [
    "Any coordinates in the shaded green area will be predicted as vegetation, while all other coordinates will be predicted as non-vegetation.\n",
    "\n",
    "How well does this decision tree work?  Are there locations where it is not working well?  Does the visualization of the decision tree boundary on the test data match what we saw in the confusion matrix?\n",
    "\n",
    "To better understand how decision trees work, let's visualize the boundaries of trees with different max depths.\n",
    "\n",
    "Note:  We will re-divide the data in training and testing data in a way that is not random (using the parameter `random_state = 7`), so that everyone will get the same results in this section.\n",
    "\n",
    "First we look at a max depth of 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "1yIVbAGr3R3W",
    "outputId": "0a8b0554-1885-4ade-a11e-c18483f6e999"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 7)\n",
    "\n",
    "tree1 = DecisionTreeClassifier(max_depth = 1)\n",
    "tree1.fit(x_train,y_train)\n",
    "\n",
    "# set up to create three plots in the same image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# visualize the data on the first plot (ax[0])\n",
    "scatter_plot_data(x_train, y_train, ax[0])\n",
    "ax[0].set_title('Training Data')\n",
    "\n",
    "#  plot the training data and decision tree boundary on the second plot (ax[1])\n",
    "plot_decision_boundary(x_train, y_train, tree1, ax[1])\n",
    "ax[1].set_title('Decision Boundary on the Training Data')\n",
    "\n",
    "\n",
    "#  plot the test data and decision tree boundary on the third plot (ax[2])\n",
    "plot_decision_boundary(x_test, y_test, tree1, ax[2])\n",
    "ax[2].set_title('Decision Boundary on the Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASS2NNeU48kW"
   },
   "source": [
    "How many boundary lines are used to make this decision boundary?\n",
    "\n",
    "Run the following code only if you used plot_tree() without an error in Lab 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "JJ0TAdJv3R3X",
    "outputId": "4978ec87-8338-45d5-c20d-9baa170ff6d9"
   },
   "outputs": [],
   "source": [
    "# only run this code if you used plot_tree() without an error in Lab 12\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(tree1, feature_names = x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwa_kQSC5_xJ"
   },
   "source": [
    "Alternatively, the decision tree is\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/megan-owen/MAT328-Techniques_in_Data_Science/main/images/Lab%2016%20decision_tree_depth_1.png\">\n",
    "\n",
    "The first node check if the x coordinate is less than 0.499.  Notice that this corresponds to the vertical decision boundary in the plot above.  If the x coordinate is less than 0.499, we follow the left branch and otherwise we follow the right branch.\n",
    "\n",
    "When the decision tree is a classifier, the last line in each node (box) tells us how many data points belong to each category.  The order of the categories is alphabetical, so in the top node, 198 of the data points are 0 and 202 of the data points are 1.  The predicted category for that node is the largest category (so 1 for the top node).\n",
    "\n",
    "Of the data points that fall into the bottom left node (because their x coordinate is less than or equal to 0.499), 90 are category 0 and 171 are category 1.  Thus this node predicts category 1 (vegetation), which is reflected in the green shading to the left of x coordinate 0.499 in the decision tree boundary above.\n",
    "\n",
    "Of the data points that fall into the bottom right node (because their x coordinate is greater than 0.499), 108 are in category 0 and 31 are in category 1.  Thus any new data point reaching this node is predicted as 0 (non-vegetation), which is reflected in points to the right of the x coordinate 0.499 not being shaded green in the decision tree boundary plot above.\n",
    "\n",
    "We now repeat this process with a tree of max depth 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "Z7u9_Jss3wBY",
    "outputId": "01c5db31-88a3-4477-b97e-63ffe4814cc7"
   },
   "outputs": [],
   "source": [
    "tree2 = DecisionTreeClassifier(max_depth = 2)\n",
    "tree2.fit(x_train,y_train)\n",
    "\n",
    "# set up to create three plots in the same image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# visualize the data on the first plot (ax[0])\n",
    "scatter_plot_data(x_train, y_train, ax[0])\n",
    "ax[0].set_title('Training Data')\n",
    "\n",
    "#  plot the training data and decision tree boundary on the second plot (ax[1])\n",
    "plot_decision_boundary(x_train, y_train, tree2, ax[1])\n",
    "ax[1].set_title('Decision Boundary on the Training Data')\n",
    "\n",
    "\n",
    "#  plot the test data and decision tree boundary on the third plot (ax[2])\n",
    "plot_decision_boundary(x_test, y_test, tree2, ax[2])\n",
    "ax[2].set_title('Decision Boundary on the Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_jbxcxEAYIA"
   },
   "source": [
    "How did the decision boundary change?\n",
    "\n",
    "If convenient, plot the decision tree or check out the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "id": "c4dAnKuK_1Ie",
    "outputId": "c7775c14-20c1-4262-957d-4365ae5bb2b1"
   },
   "outputs": [],
   "source": [
    "# only run this code if you used plot_tree() without an error in Lab 12\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (5,5), dpi=150)\n",
    "plot_tree(tree2, feature_names = x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COzsLrdq_8vJ"
   },
   "source": [
    "Alternatively, the decision tree is\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/megan-owen/MAT328-Techniques_in_Data_Science/main/images/Lab%2016%20decision_tree_depth_2.png\">\n",
    "\n",
    "The top node (box) is the same as the max depth 1 tree, and the values in the second (middle) level nodes are the same.  However, the second level nodes now each have a (different) condition on the y coordinate.  \n",
    "\n",
    "These y-coordinates conditions divide the two areas from the max depth 1 tree decision boundary in two again, to give four areas.  However, only one of these four areas is predicted to have vegetation.   This corresponds to only one of the bottom nodes having a higher number in the second position for `value` (the node with `value = [55,171]`).\n",
    "\n",
    "Finally, let's create and plot the boundary of a max depth 3 decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "bQ2XeSHMACuc",
    "outputId": "0b2536e2-b01d-4d3f-df88-87f1beee4586"
   },
   "outputs": [],
   "source": [
    "tree3 = DecisionTreeClassifier(max_depth = 3)\n",
    "tree3.fit(x_train,y_train)\n",
    "\n",
    "# set up to create three plots in the same image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# visualize the data on the first plot (ax[0])\n",
    "scatter_plot_data(x_train, y_train, ax[0])\n",
    "ax[0].set_title('Training Data')\n",
    "\n",
    "#  plot the training data and decision tree boundary on the second plot (ax[1])\n",
    "plot_decision_boundary(x_train, y_train, tree3, ax[1])\n",
    "ax[1].set_title('Decision Boundary on the Training Data')\n",
    "\n",
    "\n",
    "#  plot the test data and decision tree boundary on the third plot (ax[2])\n",
    "plot_decision_boundary(x_test, y_test, tree3, ax[2])\n",
    "ax[2].set_title('Decision Boundary on the Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFAsO_MZBYL2"
   },
   "source": [
    "How did the decision boundary change?\n",
    "\n",
    "If possible, plot the decision tree or check out the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "id": "RhEafirrBKV1",
    "outputId": "19aa1232-077d-49c3-b096-13f90132a369"
   },
   "outputs": [],
   "source": [
    "# only run this code if you used plot_tree() without an error in Lab 12\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (5,5), dpi=150)\n",
    "plot_tree(tree3, feature_names = x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbIhbcCEB2Ou"
   },
   "source": [
    "Alternatively, the decision tree is\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/megan-owen/MAT328-Techniques_in_Data_Science/main/images/Lab%2016%20decision_tree_depth_3.png\">\n",
    "\n",
    "In this decision tree, the top two levels are the same as the max depth 2 decision tree, and the values in the third level are the same for the four nodes.  \n",
    "\n",
    "The first and last nodes in the third level down only have non-vegetation data points (because the second `value` is 0), so the tree ends at these nodes.\n",
    "\n",
    "Can you figure out which lines in decision boundary the two other nodes in the third level correspond to?\n",
    "\n",
    "### Section 4: Random forests\n",
    "\n",
    "A *random forest* is the machine learning version of crowd-sourcing or \"ask the audience\" in a game show like \"Who wants to be a millionaire?\".  A random forest works by taking a bunch of different random samples of the training data and constructing the decision tree for each random sample.  To make a prediction, each of these decision trees makes its prediction and whatever is predicted by the majority of the decision trees is the prediction of the random forest.\n",
    "\n",
    "One of the advantages of the random forest is overfitting errors in the decision trees can be canceled out.\n",
    "\n",
    "For example, the following code takes a random sample of the training data, finds the decision tree of max depth 100 on it, and displays the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "jFZuX7xgBg8W",
    "outputId": "f898f44a-cfbf-4099-b7fd-d25b328db0d1"
   },
   "outputs": [],
   "source": [
    "# take a random sample with replacement\n",
    "indices = np.arange(x_train.shape[0])\n",
    "sampled_indices = np.random.choice(indices, x_train.shape[0])\n",
    "x_sample = x_train.iloc[sampled_indices]\n",
    "y_sample = y_train.iloc[sampled_indices]\n",
    "\n",
    "tree100 = DecisionTreeClassifier(max_depth = 100)\n",
    "tree100.fit(x_sample,y_sample)\n",
    "\n",
    "# set up to create three plots in the same image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# visualize the data on the first plot (ax[0])\n",
    "scatter_plot_data(x_sample, y_sample, ax[0])\n",
    "ax[0].set_title('Training Data')\n",
    "\n",
    "#  plot the training data and decision tree boundary on the second plot (ax[1])\n",
    "plot_decision_boundary(x_sample, y_sample, tree100, ax[1])\n",
    "ax[1].set_title('Decision Boundary on the Training Data')\n",
    "\n",
    "\n",
    "#  plot the test data and decision tree boundary on the third plot (ax[2])\n",
    "plot_decision_boundary(x_test, y_test, tree100, ax[2])\n",
    "ax[2].set_title('Decision Boundary on the Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Dyj6ubQNT27"
   },
   "source": [
    "Rerun the above code several times.  How does the decision boundary change?  What evidence of overfitting do you see in the decision boundary?\n",
    "\n",
    "Use the last decision tree made to make predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "l_Ax7PoPEvRd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Yi8U09Nr7I"
   },
   "source": [
    "What is the confusion matrix for these predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYoqjT31MqvB",
    "outputId": "54bab03c-3ef1-42c5-e14b-20b4e051fd28"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkLwXdfyOBCM"
   },
   "source": [
    "Based on the confusion matrices, how does this decision tree compare to the one we computed in Section 2?  Which decision tree is better?\n",
    "\n",
    "Now, let's fit a random forest with 500 decision trees, each of max depth 100, to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYRZL9jbNyff",
    "outputId": "4ea3c747-7759-4d01-8851-f1d1b2464d05"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 500, max_depth = 100)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebWUZCWuOZPP"
   },
   "source": [
    "Can you figure out how to make predictions on the test data with the random forest classifier, and then compute the confusion matrix?  Remember all scikit-learn classifiers use the same functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZPn90gtOV-f",
    "outputId": "5f543c1d-5c30-4cd4-c754-152403352264"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OON0NuUtOvbQ"
   },
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "y_test_preds_rf = rf.predict(x_test)\n",
    "confusion_matrix(y_test,y_test_preds_rf)\n",
    "</code>\n",
    "</details>\n",
    "\n",
    "Which model is better?  The random forest of 500 depth 100 decision trees, or a single depth 100 decision tree?\n",
    "\n",
    "How does the random forest compare to our decision tree in section 2?\n",
    "\n",
    "Finally, run the code below to see a plot of the decision boundary for the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "CrntZ0d6OkWH",
    "outputId": "02704a49-b1eb-476f-dcee-8c04dff8babc"
   },
   "outputs": [],
   "source": [
    "# set up to create three plots in the same image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# visualize the data on the first plot (ax[0])\n",
    "scatter_plot_data(x_train, y_train, ax[0])\n",
    "ax[0].set_title('Training Data')\n",
    "\n",
    "#  plot the training data and decision tree boundary on the second plot (ax[1])\n",
    "plot_decision_boundary(x_train, y_train, rf, ax[1])\n",
    "ax[1].set_title('Decision Boundary on the Training Data')\n",
    "\n",
    "\n",
    "#  plot the test data and decision tree boundary on the third plot (ax[2])\n",
    "plot_decision_boundary(x_test, y_test, rf, ax[2])\n",
    "ax[2].set_title('Decision Boundary on the Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRjFVgSIPjj2"
   },
   "source": [
    "What do you notice about the decision boundary of the random forest? \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 16 - Decision trees and random forest classifiers (complete).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
