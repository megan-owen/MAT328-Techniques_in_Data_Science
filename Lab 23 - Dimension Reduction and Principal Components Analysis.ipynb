{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 23 - Dimension Reduction and Principal Components Analysis\n",
    "\n",
    "*Principal Components Analysis* or *PCA* is a method for reducing the dimension of the data.  The dimension of the data is the number of columns or variables in the data.  We encounter dimension reduction in every day life every time we look at a 2D picture or photo of a 3D object.  Some ways of reducing the dimension are more useful than others.  For example, if someone takes a photo looking straight down at a table, it might be much harder to tell that it is a table than if the photo is taken at an angle so you can see the table top and all 4 table legs.  \n",
    "\n",
    "Principal Components Analysis tries to map the data to lower dimensions in such a way that as much variation as possible is retained.  Specifically, PCA reduces the dimensions of the data set by successively finding the directions with the most variation and using these directions as the new coordinate system.\n",
    "\n",
    "See [https://setosa.io/ev/principal-component-analysis/](https://setosa.io/ev/principal-component-analysis/) for a visual, interactive explaination of PCA.\n",
    "\n",
    "### Section 1: Loading and cleaning the penguin data\n",
    "\n",
    "We will start by looking at the Palmer penguin dataset from Labs 3, 21, and 22.  First load the data into a variable.\n",
    "\n",
    "Data URL: https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For technical reasons later in the lab, we need to reset the index as well.  Recall the function is `reset_index(drop = True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "<code>\n",
    "penguins = penguins.reset_index(drop = True)\n",
    "</code>\n",
    "</details>\n",
    "\n",
    "We can use the qualitative variables `species`, `island`, and `sex` as well as the quantitative variables to do PCA by first turning them into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because PCA looks for the direction with the highest variance, we need to pre-process our data so that each column has the same variance.  Otherwise a column with high variance will dominate.  The scaler we have used in previous labs does not make sure the variances are the same.\n",
    "\n",
    "Instead, we will *standardize* each column of data, by taking the *z-score* of each data point.  After standardizing a column of data, the mean of that column will be 0 and the variance and standard deviation will be 1.\n",
    "\n",
    "To do this, we create a StandardScaler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standardizer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use it to transform our data.  Notice the function is the same as for scaling, just applied to a StandardScaler object instead of a MinMaxScaler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "penguins2_standardized = standardizer.fit_transform(penguins2)\n",
    "penguins2_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Principal Components Analysis\n",
    "\n",
    "Let's find the first two principal components for our processed penguin dataset (which we assume is stored in the variable `penguins2_standardized`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# find the first two principal components of the penguin data\n",
    "pca.fit(penguins2_standardized)\n",
    "\n",
    "# find the coordinates of the penguin data in the new coordinate system\n",
    "penguins_pca = pca.transform(penguins2_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display `penguins_pca`.  Is it a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the data from `penguins_pca` (which is a numpy array) into a DataFrame, along with the species names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the coordinates into a DataFrame\n",
    "penguins_pca_df = pd.DataFrame(penguins_pca, columns = [\"PC1\",\"PC2\"])\n",
    "# add the species names as a new column\n",
    "penguins_pca_df[\"species\"] = penguins[\"species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Seaborn to plot a scatter plot where x is PC1 and y is PC2, with the points colored by the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about this plot? What variation is shown along PC1?  Along PC2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this scatter plot with the scatter plots for all pairs of quantitative variables, colored by the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these scatterplots compare to the PCA one?  Which shows the most separation between species?  Which species is most separated from the others in the scatterplots of the original quantitiative variables?  How is this reflected in the scatterplot of the PCA coordinates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3:  Cleaning and pre-processing the labor market data\n",
    "\n",
    "Let's apply PCA to the labor market data from Homeworks 21 and 22.\n",
    "\n",
    "Recall the Federal Reserve Bank of New York has information about the labor market for recent college graduates [here](https://www.newyorkfed.org/research/college-labor-market/college-labor-market_compare-majors.html).\n",
    "\n",
    "Data URL: [https://raw.githubusercontent.com/megan-owen/MAT328-Techniques_in_Data_Science/main/data/labor-market-Feb2021.csv](https://raw.githubusercontent.com/megan-owen/MAT328-Techniques_in_Data_Science/main/data/labor-market-Feb2021.csv)\n",
    "\n",
    "Load the data, skipping the first 13 rows and the last 3 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the commas in the columns `Median Wage Early Career` and `Median Wage Mid-Career`, and then change the type of the columns to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new DataFrame `x` without the `Major` column, since it is qualitative.  Since each major is different, we can't easily convert this column into quantitative data using dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a new variable `x_standardized` with the standardized columns of `x`.  That is, with the data in each column of `x` transformed so that it has mean 0 and variance 1 (and hence also standard deviation 1).\n",
    "\n",
    "You can use the same `standardizer` variable from Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Applying PCA to the labor market data\n",
    "\n",
    "Find the first two principal components using the standardized labor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame with the two new coordinates, as we did in Section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot these new coordinates.  We are not coloring them yet, as it is not clear what to color them with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the plot?  Do you see any distinct clusters?\n",
    "\n",
    "Let's use k-means clustering to cluster the standardized labor data.\n",
    "\n",
    "First use either the elbow method or the silhouette score to determine the number of clusers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "(for silhouette score method)\n",
    "<code>\n",
    "silhouette_score_list = []\n",
    "for k in range(2,11):\n",
    "    kmeans = KMeans(n_clusters = k)\n",
    "    kmeans_clusters = kmeans.fit_predict(x_standardized)\n",
    "    score = silhouette_score(x_standardized,kmeans_clusters)\n",
    "    silhouette_score_list.append(score)\n",
    "plt.plot(range(2,11), silhouette_score_list)\n",
    "</code>\n",
    "</details>\n",
    "\n",
    "Using the k you just determined, cluster the standardized labor data. k-means to cluster the scaled labor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the cluster labels to your DataFrame with the PCA coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the colored-by-cluster data using the principal component coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the clusters divided relative to the new PCA coordinates?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
